{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['thru', 'had', 'doing', 'six', 'other', 'always', 'than', 'in', 'sometimes', 'afterwards', 'toward', 'get', 'so', 'behind', 'all', 'us', 'whatever', 'seemed', 'if', 'top']\n"
     ]
    }
   ],
   "source": [
    "#Stop words\n",
    "#importing stop words from English language.\n",
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 425, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 507, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\http\\client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\http\\client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 186, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 331, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 177, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 333, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 282, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 482, in prepare_linked_requirement\n",
      "    hashes=hashes,\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 287, in unpack_url\n",
      "    hashes=hashes,\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 159, in unpack_http_url\n",
      "    link, downloader, temp_dir.path, hashes\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 303, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\utils\\ui.py\", line 160, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 39, in response_chunks\n",
      "    decode_content=False,\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 564, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 529, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 430, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='github-production-release-asset-2e65be.s3.amazonaws.com', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.45290068\n",
      "dog banana 0.254213\n",
      "cat dog 0.45290068\n",
      "cat cat 1.0\n",
      "cat banana 0.38413915\n",
      "banana dog 0.254213\n",
      "banana cat 0.38413915\n",
      "banana banana 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sowndariya\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "nlp= spacy.load('en_core_web_sm') \n",
    "tokens= nlp('dog cat banana')\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
